# AI-Modeling-Researcher_code

# Case Study A
RAG를 이용해서 지식 상담형 챗봇을 만드는 상황을 가정해 봅시다. 

## 1. 어떤 분야는 정확도가 굉장히 중요합니다. 다만 알다시피 LLM은 가끔 정확하지 못한 말을 할 때가 있습니다. (Hallucination) 어떻게 하면 챗봇의 Hallucination을 줄일 수 있을까요? 

보험, 법률 등과 같이 정확성이 중요한 분야에서는 고객에게 신뢰를 유지하기 위해 상담형 챗봇의 Hallucination (허위 정보 생성) 가능성을 최소화해야 합니다. 이를 위해 Hallucination을 효과적으로 줄일 수 있는 방법 중 하나는  RAG (Retrieval-Augmented Generation)  방식의 도입입니다.

RAG는 사용자의 질문을 기반으로 정보 검색 엔진을 통해 관련 정보를 찾아 LLM에 전달하고, LLM은 이를 바탕으로 사용자의 질문에 적합한 답변을 생성합니다. 검색 모듈을 통해 챗봇은 LLM에 없는 최신 정보와 정확도가 높은 답변을 제공할 수 있습니다. 그러나 안정된 정확도를 유지하기 위해서는 검색 엔진이 잘못된 정보를 반환할 가능성에 대비해야 합니다. 특히 지식 상담형 챗봇은 다양한 기업의 고객 지원 및 정보 제공에 사용되고 있으며,  보험 분야 에서는 금융위원회 등 공식 기관에서 제시하는 법적 기준에 따라 운영되지만, 세부 규정은 각 보험회사 마다 상이할 수 있습니다. 예를 들어,  A 보험회사 의 챗봇이 고객에게  B 보험회사 의 유사한 상품 정보를 제공한다면, 이는 사용자에게 혼란을 초래하고, A 회사에 상당한 손해를 입힐 수 있습니다.

이 문제를 해결하기 위해서는  검색 모듈에서 정보의 출처를 필터링 하고,  지식 그래프 를 활용하는 방법이 매우 효과적입니다. 검색 모듈에서 출처 필터링을 강화하여  공식 웹사이트 와  인증된 공식 정보 만을 검색 결과로 반영하도록 합니다. 또한, 정확한 정보를 제공하기 위해  지식 그래프 를 구축하여 정보를 구체적으로 구분하고 관리합니다. 예를 들어, A 회사의 챗봇이 사용자의 "A 회사의 건강보험에 대해 알려줘"라는 질문을 받으면, 지식 그래프는 "A 회사"와 "건강보험" 간의 관계를 명확히 구분하여 해당 회사와 관련된 정확한 정보를 제공합니다. 지식 그래프는 보험 상품과 보험사 간의 관계를 명확히 설정하여, 정확하고 신뢰할 수 있는 정보만을 제공할 수 있도록 돕습니다.

이와 같은 방식으로,  출처 필터링 과  지식 그래프 활용 을 통해 RAG 기반의 챗봇 시스템은  Hallucination의 가능성을 현저히 줄이고, 고객에게 정확하고 신뢰할 수 있는 정보를 제공할 수 있습니다.
 
## 2. 어떤 분야는 사용자에게 친숙한 인터페이스를 제공하는 것이 중요합니다. 따라서 지식을 제공하면서 동시에 사용자의 ChitChat(단순대화)에 대응해야 합니다. 이 경우 어떻게 시스템을 구성할 수 있을까요? 

시스템을 여러 독립적인 모듈로 나누어 각 모듈이 맡은 역할에 집중할 수 있도록 구성하는 접근 방식은 지식 제공 모듈과 ChitChat 모듈 간의 유연한 전환을 가능하게 합니다. 이와 같은 모듈화된 시스템은 사용자와의 상호작용에서 더욱 효율적이고 직관적인 경험을 제공합니다. 이를 위해서는 의도 분류(Intent Classification)와 상태 추적(Dialogue State Tracking, DST) 기능을 통해 사용자의 질문에 맞춰 적절한 모듈을 선택하고, 대화의 흐름을 원활하게 관리할 수 있는 시스템 설계가 필요합니다. 

의도 분류는 사용자의 질문을 분석하여 그 목적을 파악하고, 시스템이 이를 기반으로 적절한 응답을 제공할 수 있도록 돕는 중요한 과정입니다. 예를 들어, 사용자가 "A 보험 상품을 소개해줘"라고 묻는다면, 의도 분류 시스템은 이 질문이 지식 제공에 관련된 것임을 인식하고, 지식 제공 모듈을 활성화합니다. 반대로 "오늘 날씨 어때?"라는 질문은 ChitChat에 해당하므로, ChitChat 모듈로 전환되어 자연스러운 대화를 이어갑니다. 의도 분류는 키워드 매칭, 머신 러닝 모델 또는 심층 학습 모델을 사용하여 더 정확하게 이루어질 수 있으며, 이를 통해 사용자의 요청에 따라 적절한 모듈을 선택할 수 있습니다.

DST는 대화 중에 사용자의 목표와 상태를 추적하는 중요한 기능으로, 각 대화 턴에서 사용자의 의도를 추론하고, 그에 맞는 응답을 생성하는 데 필수적인 역할을 합니다. DST는 대화에서 발생하는 여러 가지 슬롯(slot)과 슬롯 값(slot values)을 추적하고, 이를 바탕으로 대화의 맥락을 유지하며 유연한 응답을 제공합니다. 기존 DST 접근 방식은 때때로 슬롯 값을 불투명하게 처리할 수 있으나, Chain-of-Thought Explanation (CoTE) 모델은 이를 개선하는 방법을 제시합니다. CoTE는 각 대화 턴에 대해 슬롯 값을 결정하고 그 값을 도출하는 과정에 대한 설명을 생성하여, 더 정확하고 신뢰할 수 있는 결과를 제공합니다. 사용자가 질문한 내용에 대해 CoTE는 각 모듈이 정확하게 작동할 수 있도록 대화 상태를 추적하고, 이에 맞는 슬롯 값을 생성하는 과정에서 중요한 역할을 합니다. 예를 들어, 사용자가 "A 보험사의 자동차 보험에 대해 알려줘"라고 물었을 때, CoTE는 "A 보험사"와 "자동차 보험" 간의 관계를 추적하고, 이를 바탕으로 정확한 정보를 제공할 수 있습니다. 이를 통해 시스템은 지식 제공과 자연스러운 대화를 모두 원활하게 처리할 수 있습니다.

CoTE 모델은 생성적 DST 프레임워크에 기반을 두고 있으며, 각 슬롯 값에 대한 설명을 단계적으로 생성함으로써 대화의 추론 능력을 향상시킵니다. 대화가 길어지거나 사용자의 응답이 복잡해지는 상황에서도 CoTE는 더욱 효과적인 응답을 제공하며, 이로써 사용자는 직관적이고 효율적인 경험을 할 수 있습니다. CoTE는 특히, 지식 제공 모듈과 ChitChat 모듈 간의 전환이 자연스러워지고, 각 모듈에서 제공하는 응답에 대해 명확한 추론 과정을 거쳐 사용자와의 상호작용에서 신뢰성 있는 답변을 생성할 수 있도록 도와줍니다.

결론적으로, 의도 분류와 DST 기능을 결합하여 CoTE 모델을 활용하면, 다양한 질문에 대해 지식 기반 응답과 자연스러운 대화를 원활하게 전환할 수 있으며, 각 모듈이 제공하는 응답의 신뢰성과 정확성을 보장할 수 있습니다. 이러한 접근 방식은 사용자에게 직관적이고 신뢰성 있는 경험을 제공하며, 시스템의 효율성 및 유연성을 더욱 강화합니다.

따라서, 이와 같은 모듈화된 설계는 상태 추적, 의도 분류, 설명 생성을 결합하여 사용자가 원하는 정보를 정확하게 제공하면서도, 사용자와의 상호작용에서 자연스럽고 친근한 대화를 유지할 수 있게 합니다.

## 3. 어떤 분야에서는 아주 길이가 긴 문서를 다루어야 합니다. 다만 현재의 LLM들은 대부분 8K~16K 정도의 토큰만을 지원합니다. 이 경우 어떻게 시스템을 구성할 수 있을까요?

대형 언어 모델(LLM)은 8K에서 16K 정도의 토큰 제한을 가지고 있어 긴 문서나 대규모 데이터를 처리하는 데 어려움을 겪습니다. 긴 문서를 효율적으로 처리하기 위해서는 여러 기법을 결합하여 시스템을 설계할 수 있습니다. 특히 핵심 정보 추출 및 요약, 다단계 추론, 문맥 추적 및 연결 기법을 활용하면 긴 문서에서도 신뢰성 높은 답변을 제공할 수 있습니다. 

긴 문서를 처리하는 첫 번째 방법은 핵심 정보 추출과 요약입니다. 긴 문서에서 불필요한 정보를 제거하고 핵심적인 정보만 추출하여 요약하는 방식입니다. 핵심 정보 추출은 문서에서 중요한 엔터티나 항목을 자동으로 추출하고, 요약 알고리즘을 통해 문서의 핵심 내용을 간결하게 제공합니다. 예를 들어, 보험 상품에 대한 긴 설명 중에서 조건이나 중요한 특약, 혜택 등을 추출하고 요약하여 사용자에게 제공합니다. 이를 통해 토큰 수를 절감하면서도 핵심 정보를 빠르게 제공할 수 있습니다. 

두 번째 방법은 다단계 추론을 사용하는 것입니다. 이 기법은 긴 문서를 여러 단계로 나누어 처리하고, 각 단계에서 독립적으로 정보를 추출한 후 이를 결합하여 최종 답변을 생성합니다. 예를 들어, 사용자가 자동차 보험에 대해 질문할 때, 첫 번째 단계에서는 보험 상품의 기본적인 사항을 추출하고, 두 번째 단계에서는 조건이나 특약을 추출하여 이를 결합합니다. 이렇게 함으로써 각 단계에서 오류를 최소화하고, 토큰 수를 절감하며 문서의 복잡성을 효과적으로 관리할 수 있습니다. 다단계 추론은 긴 문서를 분할하여 각 청크에서 필요한 정보를 추출한 후, 이를 일관되게 연결하는 데 매우 유용합니다.

세 번째 방법은 문맥 추적과 연결입니다. 긴 문서를 여러 개의 청크로 분할할 경우, 각 청크 간의 논리적 흐름을 유지하는 것이 매우 중요합니다. 문서 청크 간의 관계를 추적하고, 문서 내에서 중요한 정보들이 서로 연결될 수 있도록 합니다. 예를 들어, 첫 번째 청크에서 보험 상품의 개요를 설명하고, 두 번째 청크에서 특약을 설명하는 방식으로, 각 청크를 독립적으로 처리하되 문맥을 일관되게 유지합니다. 이렇게 하면 사용자는 여러 청크에서 나온 정보를 자연스럽게 이어서 이해할 수 있으며, 문맥의 일관성을 확보할 수 있습니다. 문맥 추적을 통해, 문서의 논리적인 흐름을 유지하며 정보가 올바르게 연결되도록 할 수 있습니다.

이와 같은 기법들은 긴 문서를 효과적으로 처리하는 데 중요한 역할을 합니다. 핵심 정보 추출 및 요약을 통해 중요한 정보를 간결하게 제공하고, 다단계 추론을 통해 긴 문서를 여러 단계로 분할하여, 문맥 추적 및 연결을 통해 각 청크 간의 일관성을 유지합니다. 이 세 가지 기법을 결합함으로써 긴 문서에서도 정확하고 신뢰성 있는 답변을 제공할 수 있습니다. 


## 참고 논문:

Peng, Boci, Zhu, Yun, Liu, Yongchao, Bo, Xiaohe, Shi, Haizhou, Hong, Chuntao, Zhang, Yan, & Tang, Siliang. (2024). "Graph Retrieval-Augmented Generation: A Survey".

Xu, L., Peng, N., Zhou, D., Ng, S. K., & Fu, J. (2022). "Chain of Thought Explanation for Dialogue State Tracking". Proceedings of the Conference on Neural Information Processing Systems (NeurIPS).
